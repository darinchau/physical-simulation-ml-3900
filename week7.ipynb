{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelFactory' from partially initialized module 'models_base' (most likely due to a circular import) (c:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\models_base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m dataclass\n",
      "File \u001b[1;32mc:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\models.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels_base\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load_spacing, get_device\n",
      "File \u001b[1;32mc:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\models_base.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m dataclass\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m get_device\n\u001b[0;32m     18\u001b[0m \u001b[39m# Filter all warnings\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\load.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m NDArray\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mh5py\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels_base\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelFactory\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ModelFactory' from partially initialized module 'models_base' (most likely due to a circular import) (c:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\models_base.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from models import *\n",
    "from load import *\n",
    "from dataclasses import dataclass\n",
    "import multiprocessing as mp\n",
    "from anim import make_plots, AnimationMaker, DataVisualizer\n",
    "from multiprocessing import Pool\n",
    "from trainer import Trainer\n",
    "from torch import Tensor, nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from models_base import Dataset, get_device\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from derivative import NormalizedPoissonLoss\n",
    "\n",
    "ROOT = \"./Datas/Week 7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Progress:\n",
    "    def __init__(self, pad = 100):\n",
    "        self.pad = pad\n",
    "    \n",
    "    def rint(self, content: str):\n",
    "        print(content.ljust(self.pad), end = '\\r')\n",
    "        self.pad = max(self.pad, len(content) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT = 2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1 = nn.Linear(4386, 300)\n",
    "        self.s1 = nn.Tanh()\n",
    "        self.l2 = nn.Linear(300, 50)\n",
    "        self.s2 = nn.Tanh()\n",
    "\n",
    "        self.lmu = nn.Linear(50, LATENT)\n",
    "        self.smu = nn.Tanh()\n",
    "\n",
    "        self.lsi = nn.Linear(50, LATENT)\n",
    "        self.ssi = nn.Tanh()\n",
    "\n",
    "        # Move device to cuda if possible\n",
    "        device = get_device()\n",
    "        zero = torch.tensor(0).float().to(device)\n",
    "        one = torch.tensor(1).float().to(device)\n",
    "        self.N = torch.distributions.Normal(zero, one)\n",
    "        self.kl = torch.tensor(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Linear 1 + normalization + tanh activation\n",
    "        x = self.l1(x)\n",
    "        mx = torch.max(torch.abs(x))\n",
    "        x = self.s1(x/mx)*mx\n",
    "\n",
    "        # Linear 2 + normalization + tanh\n",
    "        x = self.l2(x)\n",
    "        mx = torch.max(torch.abs(x))\n",
    "        x = self.s2(x/mx)*mx\n",
    "\n",
    "        # mu + normalization\n",
    "        mu = self.lmu(x)\n",
    "        mx = torch.max(torch.abs(mu))\n",
    "        mu = self.smu(mu/mx)*mx\n",
    "\n",
    "        # sigma + normalization + exp to make sigma positive\n",
    "        sigma = self.lsi(x)\n",
    "        mx = torch.max(torch.abs(sigma))\n",
    "        sigma = self.ssi(sigma/mx)*mx\n",
    "        sigma = torch.exp(sigma)\n",
    "\n",
    "        # z = mu + sigma * N(0, 1)\n",
    "        z = mu + sigma * self.N.sample(mu.shape)\n",
    "\n",
    "        # KL divergence\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1).sum()\n",
    "        return z\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1 = nn.Linear(LATENT, 50)\n",
    "        self.s1 = nn.Tanh()\n",
    "        self.l2 = nn.Linear(50, 300)\n",
    "        self.s2 = nn.Tanh()\n",
    "        self.l3 = nn.Linear(300, 4386)\n",
    "        self.s3 = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        mx = torch.max(torch.abs(x))\n",
    "        x = self.s1(x/mx)*mx\n",
    "\n",
    "        x = self.l2(x)\n",
    "        mx = torch.max(torch.abs(x))\n",
    "        x = self.s2(x/mx)*mx\n",
    "\n",
    "        x = self.l3(x)\n",
    "        mx = torch.max(torch.abs(x))\n",
    "        x = self.s3(x/mx)*mx\n",
    "        return x\n",
    "\n",
    "class PoissonVAE(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 1.60217663e-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 4386])\n"
     ]
    }
   ],
   "source": [
    "ep = Dataset(load_elec_potential())\n",
    "sc = Dataset(load_space_charge() * (-q))\n",
    "epsc = (ep + sc).clone().to_tensor()\n",
    "print(epsc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 4.00 GiB total capacity; 3.20 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         history\u001b[39m.\u001b[39mappend([mse_loss\u001b[39m.\u001b[39mitem(), poi_loss\u001b[39m.\u001b[39mitem(), kl_diver\u001b[39m.\u001b[39mitem()])\n\u001b[0;32m     29\u001b[0m         \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m---> 30\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[0;32m     31\u001b[0m mse_, poi_, kl_ \u001b[39m=\u001b[39m history[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m p\u001b[39m.\u001b[39mrint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mElapsed \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m epochs with MSE: \u001b[39m\u001b[39m{\u001b[39;00mmse_\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Poisson: \u001b[39m\u001b[39m{\u001b[39;00mpoi_\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, KL divergence: \u001b[39m\u001b[39m{\u001b[39;00mkl_\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lbfgs.py:461\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39m# lack of progress\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m \u001b[39mif\u001b[39;00m d\u001b[39m.\u001b[39;49mmul(t)\u001b[39m.\u001b[39;49mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_change:\n\u001b[0;32m    462\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(loss \u001b[39m-\u001b[39m prev_loss) \u001b[39m<\u001b[39m tolerance_change:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 4.00 GiB total capacity; 3.20 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "net = PoissonVAE().to(device).double()\n",
    "history = []\n",
    "mse = nn.MSELoss()\n",
    "poi = NormalizedPoissonLoss()\n",
    "epochs = 10\n",
    "epsc = epsc.to(device)\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), lr=0.01)\n",
    "p = Progress()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(epsc)):\n",
    "        x = epsc[i:i+1]\n",
    "        def closure():\n",
    "            if torch.is_grad_enabled():\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            x_hat = net(x)\n",
    "            mse_loss = mse(x, x_hat)\n",
    "            poi_loss = poi(x, x_hat)\n",
    "            kl_diver = net.encoder.kl\n",
    "            loss = mse_loss + poi_loss + kl_diver\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "\n",
    "            global history\n",
    "            history.append([mse_loss.item(), poi_loss.item(), kl_diver.item()])\n",
    "\n",
    "            global p\n",
    "            mse_, poi_, kl_ = history[-1]\n",
    "            p.rint(f\"Elapsed {epoch} epochs with MSE: {mse_:.7f}, Poisson: {poi_:.7f}, KL divergence: {kl_:.7f}\")\n",
    "\n",
    "            return loss\n",
    "        optimizer.step(closure)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
