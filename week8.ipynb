{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import *\n",
    "from torch import Tensor, nn\n",
    "import torch\n",
    "from anim import *\n",
    "from model_base import Model, Trainer\n",
    "from modules import *\n",
    "from derivative import NormalizedPoissonRMSE\n",
    "\n",
    "ROOT = \"./Datas/Week 8\"\n",
    "\n",
    "Q = 1.60217663e-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = load_space_charge() * -Q\n",
    "ep = load_elec_potential()\n",
    "vg = load_vgs()\n",
    "ma = load_materials()\n",
    "co = load_contacts()\n",
    "sx, sy = load_spacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(ep, sc):\n",
    "    xep = ep.reshape(-1, 129, 17)\n",
    "    xsc = sc.reshape(-1, 129, 17)\n",
    "            \n",
    "    ep_region_2 = xep[:, 45:84,:11].reshape(-1, 429)\n",
    "    ep_region_5 = xep[:, 45:84,11:].reshape(-1, 234)\n",
    "    sc_region_2 = xsc[:, 45:84,:11].reshape(-1, 429)\n",
    "\n",
    "    joined = torch.cat([ep_region_2, ep_region_5, sc_region_2], dim = 1)\n",
    "\n",
    "    return joined\n",
    "\n",
    "def reconstruct(x, xep, xsc):\n",
    "    ep_region_2 = x[:, :429].reshape(-1, 39, 11)\n",
    "    ep_region_5 = x[:, 429:663].reshape(-1, 39, 6)\n",
    "    sc_region_2 = x[:, 663:].reshape(-1, 39, 11)\n",
    "\n",
    "    xep = xep.clone()\n",
    "    xep[:, 45:84,:11] = ep_region_2\n",
    "    xep[:, 45:84,11:] = ep_region_5\n",
    "    xep = xep.reshape(-1, 129, 17)\n",
    "\n",
    "    xsc = xsc.clone()\n",
    "    xsc[:, 45:84,:11] = sc_region_2\n",
    "    xsc = xsc.reshape(-1, 129, 17)\n",
    "\n",
    "    return xep, xsc\n",
    "\n",
    "class PoissonFixModel(Model):\n",
    "    def __init__(self, device = None):\n",
    "        self.fc = Sequential(\n",
    "            Linear(1 + 4386 * 3, 4386),\n",
    "            LeakySigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        # This inner model should accept 1 + 4386 * 3 variables and output 4386 things\n",
    "        cached_state_ = torch.zeros(3, 4386).to(Device()).double()\n",
    "        results = torch.zeros(X.shape[0], 4386).to(Device()).double()\n",
    "\n",
    "        poi = NormalizedPoissonRMSE()\n",
    "\n",
    "        # Predict for each variable\n",
    "        for i in range(X.shape[0]):\n",
    "            # Predict\n",
    "            x_ = torch.concat([X[i], cached_state_.reshape(-1)]).reshape(1, -1)\n",
    "            x_ = self.fc(x_) # Shape should be 1, 4386\n",
    "\n",
    "            # Gradient descent\n",
    "            xep = x_[:, :2193].reshape(-1, 129, 17).detach()\n",
    "            xsc = x_[:, 2193:].reshape(-1, 129, 17).detach()\n",
    "            x = extract(xep, xsc).clone().detach()\n",
    "            x.requires_grad = True\n",
    "            optim = torch.optim.Adam([x])\n",
    "            for _ in range(20):\n",
    "                optim.zero_grad()\n",
    "                rep, rsc = reconstruct(x, xep, xsc)\n",
    "                poi_loss = poi(rep, rsc)\n",
    "                poi_loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            # Reconstruct x\n",
    "            rep, rsc = reconstruct(x, xep, xsc)\n",
    "            x_[:, :2193] = rep.reshape(-1, 2193)\n",
    "            x_[:, 2193:] = rsc.reshape(-1, 2193)\n",
    "            \n",
    "            # Store the cached state\n",
    "            cached_state_[1:] = cached_state_[:-1].clone()\n",
    "            cached_state_[0] = x_\n",
    "\n",
    "            # Store the result\n",
    "            results[i] = x_\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_base import fit\n",
    "\n",
    "class PFMTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        self.model = PoissonFixModel().to(Device())\n",
    "        self.mse = MSELoss()\n",
    "        self.poi = NormalizedPoissonRMSE()\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        x = self.model(x)\n",
    "        mse = self.mse(x, y)\n",
    "        poi = self.poi(x[:, :2193].reshape(-1, 129, 17), x[:, 2193:].reshape(-1, 129, 17))\n",
    "        self.add_loss(\"MSE\", mse.item())\n",
    "        self.add_loss(\"Poisson\", poi.item())\n",
    "        return mse + poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "On epoch 500, train loss = 15.8862016, test loss = 17.2000832: 100%|██████████| 500/500 [1:38:38<00:00, 11.84s/it]\n"
     ]
    }
   ],
   "source": [
    "idx = util.TRAINING_IDXS[\"First 30\"]\n",
    "model_ = PFMTrainer()\n",
    "y = torch.cat([ep.reshape(-1, 2193), sc.reshape(-1, 2193)], dim = 1)\n",
    "model = fit(model_, vg, y, idx, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.model(vg.to(Device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\AppData\\Local\\Temp\\ipykernel_18632\\2681282300.py:8: RuntimeWarning: overflow encountered in square\n",
      "  rmse = np.sqrt(np.mean((ypred - y) ** 2))\n"
     ]
    }
   ],
   "source": [
    "ypred = results[:, :2193].detach().cpu().numpy().reshape(-1, 129, 17)\n",
    "y = ep.detach().numpy()\n",
    "\n",
    "anim = AnimationMaker()\n",
    "anim.add_data(ep, \"Original\")\n",
    "anim.add_data(ypred, \"Predicted data\", vmin = ep.min().item(), vmax = ep.max().item())\n",
    "anim.add_data(log_diff(ep, ypred), \"Log difference\")\n",
    "rmse = np.sqrt(np.mean((ypred - y) ** 2))\n",
    "worst = np.max(np.abs(ypred - y))\n",
    "rmse_last_10_frames = np.sqrt(np.mean((ypred[-10:] - y[-10:]) ** 2))\n",
    "worst_last_10_frames = np.max(np.abs(ypred[-10:] - y[-10:]))\n",
    "\n",
    "error_text = f\"RMSE: {round(rmse, 5)}, (last 10 frames): {round(rmse_last_10_frames, 5)}, worst = {round(worst, 5)} (last 10 frames) = {round(worst_last_10_frames, 5)}\"\n",
    "anim.add_text([error_text for _ in range(101)])\n",
    "\n",
    "frames = [f\"Frame {frame} - {round(frame*0.0075, 4)}V\" for frame in range(101)]\n",
    "anim.add_text(frames)\n",
    "\n",
    "anim.save(\"./a.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save(\"./a.hlpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 4.00 GiB total capacity; 3.06 GiB already allocated; 0 bytes free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m \u001b[39m=\u001b[39m PoissonFixModel\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./a.hlpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Darin\\Documents\\Repository\\physical-simulation-ml-3900\\model_base.py:132\u001b[0m, in \u001b[0;36mModel.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the model\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 132\u001b[0m     state \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unpickel(state)\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[0;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Darin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:185\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n\u001b[1;32m--> 185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mUntypedStorage(obj\u001b[39m.\u001b[39;49mnbytes(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(location))\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mcuda(device)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 4.00 GiB total capacity; 3.06 GiB already allocated; 0 bytes free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "m = PoissonFixModel.load(\"./a.hlpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
